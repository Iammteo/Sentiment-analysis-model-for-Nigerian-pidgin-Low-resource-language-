# -*- coding: utf-8 -*-
"""2419706SWE6204.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HbdNlX4S4cHNyNWEu9kPSw1LA0yOyFa2
"""

import pandas as pd
import re
from google.colab import drive
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from tabulate import tabulate
from scipy.stats import uniform

drive.mount('/content/drive')

# Load dataset
try:
    df = pd.read_excel('/content/drive/MyDrive/Pidgin_reviews.xlsx')
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: Unable to find file , check directory  again! ")

# Function to clean text
def clean_text(text):
    if not isinstance(text, str):
        text = str(text)
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply cleaning
df["cleaned_reviews"] = df["Reviews"].apply(clean_text)

# Remove duplicates and NaN values
df.drop_duplicates(subset=["Reviews"], keep="first", inplace=True)
df = df[df["cleaned_reviews"].notna() & (df["cleaned_reviews"] != '')]

# Encode sentiment labels into numbers
label_encoder = LabelEncoder()
df['Sentiments'] = label_encoder.fit_transform(df['Sentiments'])

# Splitting data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df['cleaned_reviews'],
    df['Sentiments'], test_size=0.2, random_state=42
)

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Training SVM with linear kernel
svm_default = SVC(kernel='linear' )
svm_default.fit(X_train_tfidf, y_train)
y_pred_default = svm_default.predict(X_test_tfidf)

# Evaluate SVM with linear kernel
accuracy_default = accuracy_score(y_test, y_pred_default)
default = classification_report(y_test, y_pred_default, target_names=label_encoder.classes_, output_dict=True)
report_default = pd.DataFrame(default).transpose()
print(" SVM with linear kernel Performance")
print(tabulate(report_default, headers='keys', tablefmt='grid'))

# Training SVM with Grid search hyperparameter tuning
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'linear']
}

svm_grid = GridSearchCV(SVC(), param_grid, cv=5, verbose=1, n_jobs=-1)
svm_grid.fit(X_train_tfidf, y_train)

# Best parameters
best_svm = svm_grid.best_estimator_
print(best_svm)
y_pred_grid = best_svm.predict(X_test_tfidf)

# Evaluate Grid search SVM tuning
accuracy_tuned = accuracy_score(y_test, y_pred_grid)
tuned = classification_report(y_test, y_pred_grid, target_names=label_encoder.classes_, output_dict=True)
report_tuned = pd.DataFrame(tuned).transpose()
print(" Grid search SVM Performance")
print(tabulate(report_tuned, headers='keys', tablefmt='grid'))

# Training SVM with Randomized Search hyperparameter tuning
param_dist = {
    'C': uniform(loc=0.01, scale=10),
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],
    'gamma': ['scale', 'auto'],
    'degree': [2, 3, 4],
}

svm_random = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=30, scoring='accuracy', cv=5, verbose=1, random_state=42, n_jobs=-1)
svm_random.fit(X_train_tfidf, y_train)

# Best parameters from Randomized Search
best_svm_random = svm_random.best_estimator_
print("Best parameters from Randomized Search:", svm_random.best_params_)
y_pred_random = best_svm_random.predict(X_test_tfidf)

# Evaluate Randomized Search SVM tuning
accuracy_random = accuracy_score(y_test, y_pred_random)
random_search_report = classification_report(y_test, y_pred_random, target_names=label_encoder.classes_, output_dict=True)
report_random = pd.DataFrame(random_search_report).transpose()
print(" Randomized Search SVM Performance")
print(tabulate(report_random, headers='keys', tablefmt='grid'))

# Accuracy comparison between different SVM tuning methods
comparison = pd.DataFrame({
    "Metric": ["Accuracy"],
    "SVM tuning with linear kernel": [f"{accuracy_default * 100:.2f}%"],
    "SVM tuning with Grid search": [f"{accuracy_tuned * 100:.2f}%"],
    "SVM tuning with Randomized search": [f"{accuracy_random * 100:.2f}%"],
})

print("Comparison of Sentiment Model Performance among different SVM tuning methods :")
print(tabulate(comparison, headers='keys', tablefmt='grid'))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(title)
    plt.show()

# Plot confusion matrix for Linear kernel tuned SVM
plot_confusion_matrix(y_test, y_pred_default, "Confusion Matrix - Linear kernel tuned SVM ")

# Plot confusion matrix for Grid search tuned SVM
plot_confusion_matrix(y_test, y_pred_grid, "Confusion Matrix - Grid search tuned SVM")

#Plot confusion matrix for Random search tuned svm
plot_confusion_matrix(y_test, y_pred_random, "Confusion Matrix - Random search tuned SVM")



example = input("Enter a review:")
example = [example]
result = svm_default.predict(tfidf_vectorizer.transform(example))
print(f"This review is",label_encoder.inverse_transform(result))



